{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import configparser\n",
    "from pyspark.sql import SparkSession"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conf_parser = configparser.ConfigParser()\n",
    "with open('etl_config.cfg', 'r') as config_file:\n",
    "    conf_parser.read_file(config_file)\n",
    "\n",
    "aws_access = conf_parser['AWS']['AWS_ACCESS_KEY']\n",
    "aws_secret = conf_parser['AWS']['AWS_SECRET_KEY']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.jars.packages\",\n",
    "                                    \"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "    .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data from json files\n",
    "log_data = spark.read.json(path='log-data/*')\n",
    "song_data = spark.read.json(path='song_data/*/*/*/*.json')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "song_data.show(n=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "log_data.show(n=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "song_data.createOrReplaceTempView('song_data')\n",
    "log_data.createOrReplaceTempView('log_data')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract users table data and store in parquet format\n",
    "\n",
    "users_extract = \"\"\"\n",
    "SELECT DISTINCT userId AS user_id,\n",
    "    firstName AS first_name,\n",
    "    lastName AS last_name,\n",
    "    gender,\n",
    "    level\n",
    "FROM log_data\n",
    "\"\"\"\n",
    "users_table_data = spark.sql(users_extract)\n",
    "users_table_data.write.parquet('Transformed_data/users.parquet')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract songs table data and store it in parquet format\n",
    "\n",
    "songs_extract = \"\"\"\n",
    "SELECT DISTINCT song_id,\n",
    "    title,\n",
    "    artist_id,\n",
    "    year,\n",
    "    duration\n",
    "FROM song_data\n",
    "\"\"\"\n",
    "songs_tbl_data = spark.sql(songs_extract)\n",
    "songs_tbl_data.write.parquet(\"Transformed_data/songs.parquet\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract artists table data and store it in parquet format\n",
    "\n",
    "artists_extract = \"\"\"\n",
    "SELECT DISTINCT artist_id,\n",
    "    artist_name AS name,\n",
    "    artist_location AS location,\n",
    "    artist_latitude AS lattitude,\n",
    "    artist_longitude AS longitude\n",
    "FROM song_data\n",
    "\"\"\"\n",
    "artists_tbl_data = spark.sql(artists_extract)\n",
    "artists_tbl_data.write.parquet(\"Transformed_data/artists.parquet\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract time table data and store it in parquet format\n",
    "\n",
    "time_extract = \"\"\"\n",
    "SELECT t1.timestamp AS start_time,\n",
    "    hour(t1.timestamp) AS hour,\n",
    "    dayofmonth(t1.timestamp) AS day,\n",
    "    weekofyear(t1.timestamp) AS week,\n",
    "    month(t1.timestamp) AS month,\n",
    "    year(t1.timestamp) AS year,\n",
    "    CASE WHEN dayofweek(t1.timestamp) IN (6, 7) THEN True ELSE False END AS \n",
    "    weekday\n",
    "FROM \n",
    "    (SELECT from_unixtime(ts/1000, 'YYYY-MM-dd hh:mm:ss') AS timestamp \n",
    "    FROM log_data) t1\n",
    "\"\"\"\n",
    "time_tbl_data = spark.sql(time_extract)\n",
    "time_tbl_data.write.parquet(\"Transformed_data/time.parquet\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract songsplay table data and store it in parquet format\n",
    "songsplay_extract = \"\"\"\n",
    "SELECT log_temp.start_time,\n",
    "    log_temp.user_id,\n",
    "    log_temp.level,\n",
    "    song_temp.song_id,\n",
    "    song_temp.artist_id,\n",
    "    log_temp.session_id,\n",
    "    log_temp.location,\n",
    "    log_temp.user_agent\n",
    "FROM (SELECT from_unixtime(ts/1000, 'YYYY-MM-dd hh:mm:ss') AS start_time,\n",
    "        userId AS user_id,\n",
    "        level,\n",
    "        song,\n",
    "        artist,\n",
    "        location,\n",
    "        sessionId AS session_id,\n",
    "        userAgent AS user_agent\n",
    "    FROM log_data) log_temp\n",
    "JOIN\n",
    "    (SELECT song_id,\n",
    "        artist_id,\n",
    "        artist_name,\n",
    "        title\n",
    "    FROM song_data) song_temp\n",
    "ON log_temp.song = song_temp.title\n",
    "AND log_temp.artist = song_temp.artist_name\n",
    "\"\"\"\n",
    "songsplay_tbl = spark.sql(songsplay_extract)\n",
    "songsplay_tbl.write.parquet(\"Transformed_data/songsplay.parquet\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import configparser\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# conf_parser = configparser.ConfigParser()\n",
    "# with open('etl_config.cfg', 'r') as config_file:\n",
    "#     conf_parser.read_file(config_file)\n",
    "#\n",
    "# aws_access = conf_parser['AWS']['AWS_ACCESS_KEY']\n",
    "# aws_secret = conf_parser['AWS']['AWS_SECRET_KEY']\n",
    "#\n",
    "# spark = SparkSession.builder.config(\"spark.jars.packages\",\n",
    "#                                     \"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "#     .getOrCreate()\n",
    "#\n",
    "# log_data = spark.read.json(path='log-data/*')\n",
    "# song_data = spark.read.json(path='song_data/*/*/*/*.json')\n",
    "# log_data.show()\n",
    "# song_data.show()\n",
    "\n",
    "\n",
    "def initiate_session():\n",
    "    spark = SparkSession.builder\\\n",
    "        .config(\"spark.jars.packages\",\n",
    "                \"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "        .appName(\"sparkify_etl\")\\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "\n",
    "def load_songs_data(spark_session, input_location):\n",
    "    songs_df = spark_session.read.json(input_location)\n",
    "    return songs_df\n",
    "\n",
    "\n",
    "def load_log_data(spark_session, input_location):\n",
    "    logs_df = spark_session.read.json(input_location)\n",
    "    return logs_df\n",
    "\n",
    "\n",
    "def load_data(spark_session, songs_location, logs_location):\n",
    "    songs_df = load_songs_data(spark_session, songs_location)\n",
    "    logs_df = load_log_data(spark_session, logs_location)\n",
    "    return songs_df, logs_df\n",
    "\n",
    "\n",
    "def create_temp_table(data_frame, table_name):\n",
    "    data_frame.createOrReplaceTempView(table_name)\n",
    "\n",
    "\n",
    "def etl_songs_table(spark_session, output_location):\n",
    "    extract_song_data = \"\"\"\n",
    "    SELECT DISTINCT song_id,\n",
    "        title,\n",
    "        artist_id,\n",
    "        year,\n",
    "        duration\n",
    "    FROM song_data\n",
    "    \"\"\"\n",
    "    song_data = spark_session.sql(extract_song_data)\n",
    "    output_dir = os.path.join(output_location, \"songs.parquet\")\n",
    "    song_data.write.parquet(output_dir)\n",
    "\n",
    "\n",
    "def etl_users_table(spark_session, output_location):\n",
    "    extract_user_data = \"\"\"\n",
    "    SELECT DISTINCT userId AS user_id,\n",
    "        firstName AS first_name,\n",
    "        lastName AS last_name,\n",
    "        gender,\n",
    "        level\n",
    "    FROM log_data\n",
    "    \"\"\"\n",
    "    users_data = spark_session.sql(extract_user_data)\n",
    "    output_dir = os.path.join(output_location, \"users.parquet\")\n",
    "    users_data.write.parquet(output_dir)\n",
    "\n",
    "\n",
    "def etl_artists_table(spark_session, output_location):\n",
    "    extract_artists_data = \"\"\"\n",
    "    SELECT DISTINCT artist_id,\n",
    "        artist_name AS name,\n",
    "        artist_location AS location,\n",
    "        artist_latitude AS lattitude,\n",
    "        artist_longitude AS longitude\n",
    "    FROM song_data\n",
    "    \"\"\"\n",
    "    artists_data = spark_session.sql(extract_artists_data)\n",
    "    output_dir = os.path.join(output_location, \"artists.parquet\")\n",
    "    artists_data.write.parquet(output_dir)\n",
    "\n",
    "\n",
    "def etl_time_table(spark_session, output_location):\n",
    "    extract_time_data = \"\"\"\n",
    "    SELECT t1.timestamp AS start_time,\n",
    "        hour(t1.timestamp) AS hour,\n",
    "        dayofmonth(t1.timestamp) AS day,\n",
    "        weekofyear(t1.timestamp) AS week,\n",
    "        month(t1.timestamp) AS month,\n",
    "        year(t1.timestamp) AS year,\n",
    "        CASE WHEN dayofweek(t1.timestamp) IN (6, 7) THEN True ELSE False END AS \n",
    "        weekday\n",
    "    FROM \n",
    "        (SELECT from_unixtime(ts/1000, 'YYYY-MM-dd hh:mm:ss') AS timestamp \n",
    "        FROM log_data) t1\n",
    "    \"\"\"\n",
    "    time_data = spark_session.sql(extract_time_data)\n",
    "    output_dir = os.path.join(output_location, \"time.parquet\")\n",
    "    time_data.write.parquet(output_dir)\n",
    "\n",
    "\n",
    "def etl_songsplay_table(spark_session, output_location):\n",
    "    extract_songsplay_data = \"\"\"\n",
    "    SELECT concat(log_temp.ts, log_temp.user_id) AS songplay_id,\n",
    "        log_temp.start_time,\n",
    "        log_temp.user_id,\n",
    "        log_temp.level,\n",
    "        song_temp.song_id,\n",
    "        song_temp.artist_id,\n",
    "        log_temp.session_id,\n",
    "        log_temp.location,\n",
    "        log_temp.user_agent\n",
    "    FROM (SELECT from_unixtime(ts/1000, 'YYYY-MM-dd hh:mm:ss') AS start_time,\n",
    "            ts,\n",
    "            userId AS user_id,\n",
    "            level,\n",
    "            song,\n",
    "            artist,\n",
    "            location,\n",
    "            sessionId AS session_id,\n",
    "            userAgent AS user_agent\n",
    "        FROM log_data) log_temp\n",
    "    JOIN\n",
    "        (SELECT song_id,\n",
    "            artist_id,\n",
    "            artist_name,\n",
    "            title\n",
    "        FROM song_data) song_temp\n",
    "    ON log_temp.song = song_temp.title\n",
    "    AND log_temp.artist = song_temp.artist_name\n",
    "    \"\"\"\n",
    "    songsplay_data = spark_session.sql(extract_songsplay_data)\n",
    "    output_dir = os.path.join(output_location, \"songsplay.parquet\")\n",
    "    print(songsplay_data.show(5, truncate=False))\n",
    "    songsplay_data.write.parquet(output_dir)\n",
    "\n",
    "\n",
    "def run_etl(spark_session, output_location):\n",
    "    etl_users_table(spark_session, output_location)\n",
    "    etl_artists_table(spark_session, output_location)\n",
    "    etl_songs_table(spark_session, output_location)\n",
    "    etl_time_table(spark_session, output_location)\n",
    "    etl_songsplay_table(spark_session, output_location)\n",
    "\n",
    "\n",
    "def main():\n",
    "    spark = initiate_session()\n",
    "    songs_location = \"song_data/*/*/*/*.json\"\n",
    "    logs_location = \"log-data/*\"\n",
    "    output_dir = \"test_transform\"\n",
    "    songs_df, logs_df = load_data(spark, songs_location, logs_location)\n",
    "    create_temp_table(songs_df, \"song_data\")\n",
    "    create_temp_table(logs_df, \"log_data\")\n",
    "    run_etl(spark, output_dir)\n",
    "    print(\"ETL Completed\")\n",
    "    return spark\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}